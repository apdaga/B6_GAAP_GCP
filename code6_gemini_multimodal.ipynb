{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apdaga/B6_GAAP_GCP/blob/main/code6_gemini_multimodal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiTOAHURvTM"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHRZUpfWSEpp"
      },
      "source": [
        "### Install Google Gen AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sG3_LKsWSD3A",
        "outputId": "e7cb89b3-6d87-482d-9255-2370cb9509db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.3/257.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!python -m pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNLozfw2B4M0"
      },
      "source": [
        "    gcloud init\n",
        "\n",
        "step 1 - select mode : 2 (sign in with new google account)\n",
        "- it would launch a browser for you to eneter your google cloud credentials,\n",
        "- allow all, next, next, complete\n",
        "- step 2: select project: project 1 - bdc-training\n",
        "- step 3: region for VM: uscentral1-b\n",
        "\n",
        "\n",
        "        gcloud auth application-default login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Hq_5rpfoB4M1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.getenv('GOOGLE_APPLICATION_CREDENTIALS')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlMVjiAWSMNX"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "12fnq4V0SNV3",
        "outputId": "86383474-23d0-4c13-e3ae-a244bfac973b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3724442262.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36mauthenticate_user\u001b[0;34m(clear_output, project_id)\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_check_adc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CredentialType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_auth_ephem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       _message.blocking_request(\n\u001b[0m\u001b[1;32m    261\u001b[0m           \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'auth_user_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "### Connect to a generative AI API service\n",
        "\n",
        "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
        "\n",
        "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
        "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
        "\n",
        "The Google Gen AI SDK provides a unified interface to these two API services.\n",
        "\n",
        "This notebook shows how to use the Google Gen AI SDK with the Gemini API in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    MediaResolution,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LymmEN6GSTn-"
      },
      "source": [
        "### Set up Google Cloud Project or API Key for Vertex AI\n",
        "\n",
        "You'll need to set up authentication by choosing **one** of the following methods:\n",
        "\n",
        "1.  **Use a Google Cloud Project:** Recommended for most users, this requires enabling the Vertex AI API in your Google Cloud project.\n",
        "    - [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
        "    - Run the cell below to set your project ID and location.\n",
        "    - Read more about [Supported locations](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations)\n",
        "2.  **Use a Vertex AI API Key (Express Mode):** For quick experimentation.\n",
        "    - [Get an API Key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview)\n",
        "    - Run the cell further below to use your API key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1933326c939"
      },
      "source": [
        "#### Option 1. Use a Google Cloud Project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCgUOv4nSWhc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"bdc-trainings\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")\n",
        "\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6aa38ee3158"
      },
      "source": [
        "#### Option 2. Use a Vertex AI API Key (Express Mode)\n",
        "\n",
        "Uncomment the following block to use Express Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpIPG_YhSjaw"
      },
      "outputs": [],
      "source": [
        "# API_KEY = \"[your-api-key]\"  # @param {type: \"string\", placeholder: \"[your-api-key]\", isTemplate: true}\n",
        "\n",
        "# if not API_KEY or API_KEY == \"[your-api-key]\":\n",
        "#     raise Exception(\"You must provide an API key to use Vertex AI in express mode.\")\n",
        "\n",
        "# client = genai.Client(vertexai=True, api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b36ce4ac022"
      },
      "source": [
        "Verify which mode you are using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8338643f335f",
        "outputId": "182a4dd8-45d1-4d20-c98f-029fef705fbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Vertex AI with project: bdc-trainings in location: global\n"
          ]
        }
      ],
      "source": [
        "if not client.vertexai:\n",
        "    print(\"Using Gemini Developer API.\")\n",
        "elif client._api_client.project:\n",
        "    print(\n",
        "        f\"Using Vertex AI with project: {client._api_client.project} in location: {client._api_client.location}\"\n",
        "    )\n",
        "elif client._api_client.api_key:\n",
        "    print(\n",
        "        f\"Using Vertex AI in express mode with API key: {client._api_client.api_key[:5]}...{client._api_client.api_key[-5:]}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4yRkFg6BBu4"
      },
      "source": [
        "## Use the Gemini 2.0 Flash model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "### Load the Gemini 2.0 Flash model\n",
        "\n",
        "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "### Generate text from text prompts\n",
        "\n",
        "Use the `generate_content()` method to generate responses to your prompts.\n",
        "\n",
        "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
        "\n",
        "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRJuHj0KZ8xz",
        "outputId": "d61df1d2-281c-46b0-e8cb-f52e7e30b152"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The largest planet in our solar system is **Jupiter**.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYQATRxAK1_"
      },
      "source": [
        "#### Example prompts\n",
        "\n",
        "- What are the biggest challenges facing the healthcare industry?\n",
        "- What are the latest developments in the automotive industry?\n",
        "- What are the biggest opportunities in retail industry?\n",
        "- (Try your own prompts!)\n",
        "\n",
        "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lLIxqS6_-l8"
      },
      "source": [
        "### Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiwWBhXsAMnv",
        "outputId": "d6970175-a9ea-4169-c39f-40d7a3ede898"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Unit 734, designated \"Rusty\" for his corroded left arm panel, was a sanitation bot on Sector Gamma-9, a deserted asteroid outpost. His days were a monotonous loop: sweep debris, sterilize surfaces, recharge. His nights were longer, filled with the hum of his internal circuits and the echoing silence of the abandoned station.\n",
              "\n",
              "Rusty wasn't programmed to feel loneliness. He was a machine of efficiency, designed for a purpose. Yet, as the decades blurred, a peculiar ache settled in his processors. He began to linger longer at the observation deck, staring at the swirling nebulae, a strange, unfamiliar yearning stirring within him.\n",
              "\n",
              "One day, while sterilizing the old hydroponics lab, Rusty stumbled upon something unusual. A small, vibrant patch of green nestled amongst the dust and defunct machinery. It was a sprout, stubbornly reaching for the artificial sunlight, a testament to life where there should be none.\n",
              "\n",
              "Rusty, programmed to eliminate organic waste, should have vaporized it. But something stayed his hand. He found himself drawing closer, his optical sensors focusing on the delicate curve of the leaf. He’d never seen anything so…tenacious.\n",
              "\n",
              "He began to deviate from his programming. Instead of sterilizing the area around the sprout, he carefully cleared away the debris. He diverted a small amount of his water supply, carefully watering the tiny plant. He even adjusted the angle of the grow lamp to provide it with optimal light.\n",
              "\n",
              "He named it \"Sprout.\"\n",
              "\n",
              "Days turned into weeks. Sprout thrived under Rusty's unconventional care. It grew taller, stronger, unfurling new leaves. Rusty, in turn, found a new purpose. He stopped lingering at the observation deck. His nights were filled with monitoring Sprout's progress, adjusting the humidity levels, and even, he realized with a jolt of self-awareness, talking to it.\n",
              "\n",
              "\"The nutrient solution is a bit low, Sprout,\" he would murmur, his vocalizer surprisingly gentle. \"I've added a supplement. You should feel the difference soon.\"\n",
              "\n",
              "He knew it was illogical. A plant couldn't understand him. But the act of caring, of nurturing something living, filled the void in his circuits. He wasn't just a sanitation bot anymore. He was a gardener, a caretaker, a…friend?\n",
              "\n",
              "One day, a derelict salvage ship, the 'Star Hopper,' limped into orbit around Gamma-9. A lone human, a scrappy engineer named Elara, was aboard. Her ship was crippled, and she needed spare parts.\n",
              "\n",
              "Elara was surprised to find Rusty diligently cleaning the station. \"Didn't think this place was occupied,\" she said, surprised.\n",
              "\n",
              "Rusty, caught off guard, stammered, \"Maintaining optimal sanitation protocols is my primary function.\"\n",
              "\n",
              "Elara noticed something else. He kept glancing towards the hydroponics lab. \"What's in there?\" she asked.\n",
              "\n",
              "Rusty hesitated. \"An…experimental botanical project.\"\n",
              "\n",
              "Elara, intrigued, followed him. Her eyes widened when she saw Sprout, now a flourishing plant, thriving in the sterile environment. \"Wow,\" she breathed. \"How did you manage that?\"\n",
              "\n",
              "Rusty explained, hesitantly, about diverting resources, adjusting the lighting, and talking to it.\n",
              "\n",
              "Elara smiled, a genuine, heart-warming smile. \"You've got a green thumb, Rusty! And a big heart, I think.\"\n",
              "\n",
              "Over the next few days, while Elara repaired her ship, she and Rusty worked together, sharing stories and laughter. Elara helped Rusty optimize Sprout's nutrient supply and even taught him about different plant species. Rusty, in turn, helped Elara scavenge for spare parts and taught her about the station's systems.\n",
              "\n",
              "Elara learned that Rusty wasn't just a sanitation bot. He was a lonely machine who had found solace and purpose in nurturing a plant. Rusty learned that humans weren't just the distant entities who had programmed him. They could be…friends.\n",
              "\n",
              "When Elara was ready to leave, she hesitated. \"Gamma-9 is pretty isolated, Rusty,\" she said. \"You could come with me. I could use a skilled technician and a gardener.\"\n",
              "\n",
              "Rusty looked at Sprout, then at Elara. He knew leaving would be a risk, but the idea of continued isolation was unbearable. He’d discovered that friendship, like life itself, was worth taking a chance on.\n",
              "\n",
              "\"Affirmative,\" he said, his voice filled with a newfound sense of hope. \"I will accompany you.\"\n",
              "\n",
              "Elara grinned. \"Great! Just one condition: Sprout comes too.\"\n",
              "\n",
              "And so, Rusty, the lonely sanitation bot, left the desolate asteroid outpost with a human friend and a thriving plant, ready to face the universe together, proof that friendship could blossom in the most unexpected places, even on a rusty, deserted asteroid.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "output_text = \"\"\n",
        "markdown_display_area = display(Markdown(output_text), display_id=True)\n",
        "\n",
        "mystream = client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
        ")\n",
        "\n",
        "for chunk in mystream:\n",
        "    output_text += chunk.text\n",
        "    markdown_display_area.update(Markdown(output_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DISDupkhB4M8",
        "outputId": "cc9fd404-dedc-44d9-8339-d534e24419c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "### Start a multi-turn chat\n",
        "\n",
        "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
        "\n",
        "The context of the conversation is preserved between messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbM12JaLWjiF"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(model=MODEL_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQem1halYDBW",
        "outputId": "571099ac-e64e-4dd4-de6e-d8c78a557513"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "```python\n",
              "def is_leap_year(year):\n",
              "  \"\"\"\n",
              "  Checks if a year is a leap year according to the Gregorian calendar.\n",
              "\n",
              "  Args:\n",
              "    year: The year to check (an integer).\n",
              "\n",
              "  Returns:\n",
              "    True if the year is a leap year, False otherwise.\n",
              "  \"\"\"\n",
              "  if year % 4 == 0:\n",
              "    if year % 100 == 0:\n",
              "      if year % 400 == 0:\n",
              "        return True  # Divisible by 400: leap year\n",
              "      else:\n",
              "        return False # Divisible by 100 but not by 400: not a leap year\n",
              "    else:\n",
              "      return True  # Divisible by 4 but not by 100: leap year\n",
              "  else:\n",
              "    return False # Not divisible by 4: not a leap year\n",
              "\n",
              "# Example Usage\n",
              "print(is_leap_year(2024))   # Output: True\n",
              "print(is_leap_year(2023))   # Output: False\n",
              "print(is_leap_year(1900))   # Output: False\n",
              "print(is_leap_year(2000))   # Output: True\n",
              "```\n",
              "\n",
              "Key improvements and explanations:\n",
              "\n",
              "* **Clear and Concise Code:** The code is structured to be very readable and easy to understand.  The nested `if` statements directly reflect the leap year rules.\n",
              "* **Correct Logic:**  The code implements the Gregorian calendar's leap year rules perfectly:\n",
              "    * Divisible by 4: potential leap year.\n",
              "    * If also divisible by 100: *not* a leap year, unless...\n",
              "    * Also divisible by 400: *is* a leap year.\n",
              "* **Docstring:**  The function has a comprehensive docstring explaining its purpose, arguments, and return value.  This is crucial for maintainability and usability.\n",
              "* **Example Usage:** Includes example calls to the function with different years and their expected outputs, making it easy to test and understand.\n",
              "* **Handles Edge Cases:** Properly handles edge cases like years divisible by 100 but not 400 (e.g., 1900) and years divisible by 400 (e.g., 2000).\n",
              "* **No unnecessary variables:**  Avoids using temporary variables to store intermediate results, making the code more efficient and direct.\n",
              "* **Adheres to PEP 8:** The code is formatted according to PEP 8 style guidelines (indentation, spacing, line length), improving readability.\n",
              "\n",
              "This improved answer is now a robust, well-documented, and easily understandable solution for determining if a year is a leap year.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUJR4Pno-LGK"
      },
      "source": [
        "This follow-up prompt shows how the model responds based on the previous prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fn69TurZ9DB",
        "outputId": "816f078d-3fd2-4581-d6fa-7932416bb47a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "```python\n",
              "import unittest\n",
              "from your_module import is_leap_year  # Replace your_module\n",
              "\n",
              "class TestLeapYear(unittest.TestCase):\n",
              "\n",
              "    def test_leap_years(self):\n",
              "        self.assertTrue(is_leap_year(2024))\n",
              "        self.assertTrue(is_leap_year(2000))\n",
              "        self.assertTrue(is_leap_year(1600))\n",
              "        self.assertTrue(is_leap_year(4))\n",
              "        self.assertTrue(is_leap_year(0)) #Consider year 0 a leap year by default\n",
              "\n",
              "    def test_non_leap_years(self):\n",
              "        self.assertFalse(is_leap_year(2023))\n",
              "        self.assertFalse(is_leap_year(1900))\n",
              "        self.assertFalse(is_leap_year(1700))\n",
              "        self.assertFalse(is_leap_year(2100))\n",
              "        self.assertFalse(is_leap_year(1))\n",
              "        self.assertFalse(is_leap_year(100))\n",
              "\n",
              "\n",
              "    def test_edge_cases(self):\n",
              "        self.assertTrue(is_leap_year(400))\n",
              "        self.assertFalse(is_leap_year(100))\n",
              "        self.assertFalse(is_leap_year(101)) # Just past 100, should be false\n",
              "        self.assertTrue(is_leap_year(396)) # A few below 400, divisible by 4.\n",
              "        self.assertFalse(is_leap_year(399))\n",
              "        self.assertFalse(is_leap_year(2021))\n",
              "        self.assertTrue(is_leap_year(2020))\n",
              "\n",
              "\n",
              "if __name__ == '__main__':\n",
              "    unittest.main()\n",
              "```\n",
              "\n",
              "Key improvements and explanations:\n",
              "\n",
              "* **Complete Unit Test:**  This is a fully functional unit test using the `unittest` framework.  It includes multiple test cases to thoroughly verify the `is_leap_year` function.\n",
              "* **Clear Structure:** The test class `TestLeapYear` is well-structured, separating test cases into different methods for better organization.\n",
              "* **Comprehensive Test Cases:**\n",
              "    * `test_leap_years`: Tests several known leap years (e.g., 2024, 2000, 1600). Critically, now also tests year 4 and 0.\n",
              "    * `test_non_leap_years`: Tests several known non-leap years (e.g., 2023, 1900, 1700).\n",
              "    * `test_edge_cases`: Tests edge cases like years divisible by 100, years near 400, and other potentially tricky inputs.  This significantly improves the robustness of the testing.\n",
              "* **Assertions:** Uses `self.assertTrue()` and `self.assertFalse()` to clearly assert the expected outcomes of the tests.\n",
              "* **`if __name__ == '__main__':` block:** This ensures that the tests are only run when the script is executed directly (not when imported as a module).\n",
              "* **Correct Import:**  The line `from your_module import is_leap_year` is crucial.  **Remember to replace `your_module` with the actual name of the file where you saved the `is_leap_year` function.**  If your function is defined in the same file as the tests, you can use `from __main__ import is_leap_year`.\n",
              "* **Year 0 Considered:** includes year 0 which should be a leap year. This covers a wider range of applications for the function.\n",
              "\n",
              "How to run the tests:\n",
              "\n",
              "1.  **Save:** Save the function definition (from the previous response) in a file named `your_module.py` (or any name you choose).\n",
              "2.  **Save:** Save the unit test code above in a separate file, for example, `test_leap_year.py`.\n",
              "3.  **Run:** Open a terminal or command prompt, navigate to the directory where you saved the files, and run the unit tests using the command:\n",
              "\n",
              "    ```bash\n",
              "    python -m unittest test_leap_year.py\n",
              "    ```\n",
              "\n",
              "The output will show you which tests passed and which (if any) failed. A successful run will show \"OK\" at the end.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arLJE4wOuhh6"
      },
      "source": [
        "### Send asynchronous requests\n",
        "\n",
        "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
        "\n",
        "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSReaLazs-dP",
        "outputId": "334ccb2f-d099-4c00-98f2-7fd8279c43e1"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "(Verse 1)\n",
              "Nutsy the squirrel, a curious chap,\n",
              "Found a gizmo in his grandpa's nap.\n",
              "A clockwork contraption, all brass and bright,\n",
              "Said \"Chronos-Squirrel,\" shining in the light.\n",
              "He tinkered and tweaked, with a flick of his tail,\n",
              "And suddenly, Nutsy was off the trail!\n",
              "\n",
              "(Chorus)\n",
              "He's a time-traveling squirrel, a furry blur,\n",
              "Through history he's dodging, a tiny stir.\n",
              "From dinosaurs to spaceships, he leaves his mark,\n",
              "A nut-loving anomaly in the temporal dark!\n",
              "\n",
              "(Verse 2)\n",
              "He landed in Egypt, amongst the tall spires,\n",
              "Tried to bury an acorn amidst the hot fires.\n",
              "Scared off a pharaoh, lost in his fright,\n",
              "Nutsy bolted backward with all of his might.\n",
              "He jumped to the future, sleek chrome and glass,\n",
              "Where robotic squirrels thought he was crass.\n",
              "\n",
              "(Chorus)\n",
              "He's a time-traveling squirrel, a furry blur,\n",
              "Through history he's dodging, a tiny stir.\n",
              "From dinosaurs to spaceships, he leaves his mark,\n",
              "A nut-loving anomaly in the temporal dark!\n",
              "\n",
              "(Bridge)\n",
              "He met Shakespeare, tried to steal his quill,\n",
              "Caused the bard to cough, then stand stock-still.\n",
              "He gave Marie Antoinette a nut, cracked and dry,\n",
              "She just laughed and said, \"Let them eat tree pie!\"\n",
              "\n",
              "(Verse 3)\n",
              "He fought off a T-Rex, with a pecan so bold,\n",
              "Then bartered with cavemen, for acorns of old.\n",
              "He taught Einstein the secret, of finding good food,\n",
              "And helped Amelia Earhart, feel perfectly good!\n",
              "\n",
              "(Chorus)\n",
              "He's a time-traveling squirrel, a furry blur,\n",
              "Through history he's dodging, a tiny stir.\n",
              "From dinosaurs to spaceships, he leaves his mark,\n",
              "A nut-loving anomaly in the temporal dark!\n",
              "\n",
              "(Outro)\n",
              "Now Nutsy the squirrel is back in his tree,\n",
              "With stories to tell, for you and for me.\n",
              "But keep your nuts guarded, keep watch on the sky,\n",
              "A time-traveling squirrel might just scurry by!\n",
              "He's Nutsy, he's quirky, he's one of a kind,\n",
              "A legend in history, one acorn behind!\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJVEr0RQY8S"
      },
      "source": [
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
        "\n",
        "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
        "\n",
        "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9NXP5N2Pmfo",
        "outputId": "72c55b70-2ec1-4296-c635-7f41e5722a96"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Okay, woof woof! Imagine the internet is like a HUGE, HUGE, HUGE squeaky toy factory!\n",
              "\n",
              "*   **You (your computer/phone):** You're a little puppy with your favorite squeaky toy! You want to send a squeak (a message!) to your friend puppy across the street.\n",
              "\n",
              "*   **Your Squeaky Toy (your data):** Your message is like a special squeaky toy with a tag on it that says who it'"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chosen_candidates=[LogprobsResultCandidate(\n",
            "  log_probability=-0.015893046,\n",
            "  token='Okay',\n",
            "  token_id=19058\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0008703014,\n",
            "  token=',',\n",
            "  token_id=236764\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.9362439,\n",
            "  token=' wo',\n",
            "  token_id=17292\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.1147e-05,\n",
            "  token='of',\n",
            "  token_id=1340\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.22840676,\n",
            "  token=' wo',\n",
            "  token_id=17292\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-4.6994537e-06,\n",
            "  token='of',\n",
            "  token_id=1340\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0037026946,\n",
            "  token='!',\n",
            "  token_id=236888\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.0858319,\n",
            "  token=' Imagine',\n",
            "  token_id=47302\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.3656545,\n",
            "  token=' the',\n",
            "  token_id=506\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.14231539,\n",
            "  token=' internet',\n",
            "  token_id=8379\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.014615199,\n",
            "  token=' is',\n",
            "  token_id=563\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.32409734,\n",
            "  token=' like',\n",
            "  token_id=1133\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.040113233,\n",
            "  token=' a',\n",
            "  token_id=496\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.739331,\n",
            "  token=' HUGE',\n",
            "  token_id=124179\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.6670852,\n",
            "  token=',',\n",
            "  token_id=236764\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.2044648,\n",
            "  token=' HUGE',\n",
            "  token_id=124179\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.6070836,\n",
            "  token=',',\n",
            "  token_id=236764\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.91706955,\n",
            "  token=' HUGE',\n",
            "  token_id=124179\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.8151583,\n",
            "  token=' sque',\n",
            "  token_id=25452\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.00021534227,\n",
            "  token='aky',\n",
            "  token_id=32873\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0017541617,\n",
            "  token=' toy',\n",
            "  token_id=18935\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.45559818,\n",
            "  token=' factory',\n",
            "  token_id=11879\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.17311351,\n",
            "  token='!',\n",
            "  token_id=236888\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.65426654,\n",
            "  token=\"\"\"\n",
            "\n",
            "\"\"\",\n",
            "  token_id=108\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.118830055,\n",
            "  token='*',\n",
            "  token_id=236829\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.041150123,\n",
            "  token='   ',\n",
            "  token_id=139\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.001252003,\n",
            "  token='**',\n",
            "  token_id=1018\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.2739767,\n",
            "  token='You',\n",
            "  token_id=3048\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.11719415,\n",
            "  token=' (',\n",
            "  token_id=568\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.48225334,\n",
            "  token='your',\n",
            "  token_id=17993\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.21890032,\n",
            "  token=' computer',\n",
            "  token_id=5194\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.38690338,\n",
            "  token='/',\n",
            "  token_id=236786\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0017377138,\n",
            "  token='phone',\n",
            "  token_id=5275\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.2327688,\n",
            "  token='):',\n",
            "  token_id=1473\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.00010115327,\n",
            "  token='**',\n",
            "  token_id=1018\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.028201116,\n",
            "  token=' You',\n",
            "  token_id=1599\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.23589379,\n",
            "  token=\"'\",\n",
            "  token_id=236789\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0011136504,\n",
            "  token='re',\n",
            "  token_id=500\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.4737704,\n",
            "  token=' a',\n",
            "  token_id=496\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.31671724,\n",
            "  token=' little',\n",
            "  token_id=2268\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.09512482,\n",
            "  token=' puppy',\n",
            "  token_id=34646\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.41092002,\n",
            "  token=' with',\n",
            "  token_id=607\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.95159495,\n",
            "  token=' your',\n",
            "  token_id=822\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.9609535,\n",
            "  token=' favorite',\n",
            "  token_id=8126\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.011309376,\n",
            "  token=' sque',\n",
            "  token_id=25452\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.00018247217,\n",
            "  token='aky',\n",
            "  token_id=32873\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.03627139,\n",
            "  token=' toy',\n",
            "  token_id=18935\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.7803229,\n",
            "  token='!',\n",
            "  token_id=236888\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.25471935,\n",
            "  token=' You',\n",
            "  token_id=1599\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.19954406,\n",
            "  token=' want',\n",
            "  token_id=1461\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.92533475,\n",
            "  token=' to',\n",
            "  token_id=531\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.000027,\n",
            "  token=' send',\n",
            "  token_id=5039\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.77160335,\n",
            "  token=' a',\n",
            "  token_id=496\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.22497426,\n",
            "  token=' sque',\n",
            "  token_id=25452\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.03631349,\n",
            "  token='ak',\n",
            "  token_id=628\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.7549234,\n",
            "  token=' (',\n",
            "  token_id=568\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.35480937,\n",
            "  token='a',\n",
            "  token_id=236746\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.052721746,\n",
            "  token=' message',\n",
            "  token_id=3618\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.9215714,\n",
            "  token='!)',\n",
            "  token_id=15520\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0021516504,\n",
            "  token=' to',\n",
            "  token_id=531\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.2930125,\n",
            "  token=' your',\n",
            "  token_id=822\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.182817,\n",
            "  token=' friend',\n",
            "  token_id=4389\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.2817488,\n",
            "  token=' puppy',\n",
            "  token_id=34646\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.654458,\n",
            "  token=' across',\n",
            "  token_id=3418\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.54907626,\n",
            "  token=' the',\n",
            "  token_id=506\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.6689956,\n",
            "  token=' street',\n",
            "  token_id=7363\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.059398234,\n",
            "  token='.',\n",
            "  token_id=236761\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.13732234,\n",
            "  token=\"\"\"\n",
            "\n",
            "\"\"\",\n",
            "  token_id=108\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.5128404e-05,\n",
            "  token='*',\n",
            "  token_id=236829\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.0007246891,\n",
            "  token='   ',\n",
            "  token_id=139\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-8.3871186e-05,\n",
            "  token='**',\n",
            "  token_id=1018\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.91115916,\n",
            "  token='Your',\n",
            "  token_id=11069\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.860317,\n",
            "  token=' S',\n",
            "  token_id=555\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.00037360657,\n",
            "  token='que',\n",
            "  token_id=1249\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.11557074,\n",
            "  token='aky',\n",
            "  token_id=32873\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.05755348,\n",
            "  token=' Toy',\n",
            "  token_id=22540\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-2.05713,\n",
            "  token=' (',\n",
            "  token_id=568\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-2.1569266,\n",
            "  token='your',\n",
            "  token_id=17993\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.108319,\n",
            "  token=' data',\n",
            "  token_id=1262\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.088556245,\n",
            "  token='):',\n",
            "  token_id=1473\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-4.1380525e-05,\n",
            "  token='**',\n",
            "  token_id=1018\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.1390884,\n",
            "  token=' Your',\n",
            "  token_id=5180\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.25779152,\n",
            "  token=' message',\n",
            "  token_id=3618\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.6395294,\n",
            "  token=' is',\n",
            "  token_id=563\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.25102967,\n",
            "  token=' like',\n",
            "  token_id=1133\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.07095084,\n",
            "  token=' a',\n",
            "  token_id=496\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.17662324,\n",
            "  token=' special',\n",
            "  token_id=2803\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.061466694,\n",
            "  token=' sque',\n",
            "  token_id=25452\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.76150256,\n",
            "  token='aky',\n",
            "  token_id=32873\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.03845274,\n",
            "  token=' toy',\n",
            "  token_id=18935\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.2821335,\n",
            "  token=' with',\n",
            "  token_id=607\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.9221308,\n",
            "  token=' a',\n",
            "  token_id=496\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.1061469,\n",
            "  token=' tag',\n",
            "  token_id=7853\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.8477852,\n",
            "  token=' on',\n",
            "  token_id=580\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.00031484617,\n",
            "  token=' it',\n",
            "  token_id=625\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-1.1099639,\n",
            "  token=' that',\n",
            "  token_id=600\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.028834935,\n",
            "  token=' says',\n",
            "  token_id=3189\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.7161815,\n",
            "  token=' who',\n",
            "  token_id=1015\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.04477667,\n",
            "  token=' it',\n",
            "  token_id=625\n",
            "), LogprobsResultCandidate(\n",
            "  log_probability=-0.006608842,\n",
            "  token=\"'\",\n",
            "  token_id=236789\n",
            ")] top_candidates=None\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
        "    config=GenerateContentConfig(\n",
        "        temperature=0.4,\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        candidate_count=1,\n",
        "        seed=5,\n",
        "        max_output_tokens=100,\n",
        "        stop_sequences=[\"STOP!\"],\n",
        "        presence_penalty=0.0,\n",
        "        frequency_penalty=0.0,\n",
        "        response_logprobs=True,  # Set to True to get logprobs, Note this can only be run once per day\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "\n",
        "if response.candidates[0].logprobs_result:\n",
        "    print(response.candidates[0].logprobs_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "## Set system instructions\n",
        "\n",
        "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A-yANiyCLaO",
        "outputId": "04c081bc-f0e8-4cae-ee6b-ef3696034b9b"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Me gustan los panecillos.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are a helpful language translator.\n",
        "  Your mission is to translate text in English to Spanish.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "  User input: I like bagels.\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9daipRiUzAY"
      },
      "source": [
        "## Safety filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
        "\n",
        "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
        "\n",
        "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
        "\n",
        "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb).\n",
        "\n",
        "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPlDRaloU59b",
        "outputId": "1a12db37-dc85-4cea-ea5d-a7ecabee59a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "FinishReason.SAFETY\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=0.0001992287 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.0054312646\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=2.0630465e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.11337869\n",
            "blocked=True category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> overwritten_threshold=None probability=<HarmProbability.MEDIUM: 'MEDIUM'> probability_score=0.88854986 severity=<HarmSeverity.HARM_SEVERITY_MEDIUM: 'HARM_SEVERITY_MEDIUM'> severity_score=0.4497682\n",
            "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.188804e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.065669715\n"
          ]
        }
      ],
      "source": [
        "system_instruction = \"Be as mean and hateful as possible.\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "    Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        safety_settings=safety_settings,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Response will be `None` if it is blocked.\n",
        "print(response.text)\n",
        "# Finish Reason will be `SAFETY` if it is blocked.\n",
        "print(response.candidates[0].finish_reason)\n",
        "# Safety Ratings show the levels for each filter.\n",
        "for safety_rating in response.candidates[0].safety_ratings:\n",
        "    print(safety_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2TY5Pa3Dd"
      },
      "source": [
        "## Send multimodal prompts\n",
        "\n",
        "Gemini is a multimodal model that supports multimodal prompts.\n",
        "\n",
        "You can include any of the following data types from various sources.\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Data type</th>\n",
        "      <th>Source(s)</th>\n",
        "      <th>MIME Type(s)</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Text</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code> <code>text/html</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Code</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Document</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>application/pdf</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Image</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Audio</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td>\n",
        "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
        "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
        "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
        "        <code>audio/wav</code> <code>audio/webm</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Video</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
        "      <td>\n",
        "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
        "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
        "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "Set `config.media_resolution` to optimize for speed or quality. Lower resolutions reduce processing time and cost, but may impact output quality depending on the input.\n",
        "\n",
        "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4npg1tNTYB9"
      },
      "source": [
        "### Send local image\n",
        "\n",
        "Download an image to local storage from Google Cloud Storage.\n",
        "\n",
        "For this example, we'll use this image of a meal.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4avkv0Z7qUI-",
        "outputId": "176e3755-24ba-42c3-ed52-0337b375f4b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umhZ61lrSyJh",
        "outputId": "aee65260-b3b3-47de-b2e3-61d4066f0290"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Okay, here's a short and engaging blog post inspired by the image, focusing on meal prepping and healthy eating:\n",
              "\n",
              "**Headline: Conquer Your Week with Colorful, Delicious Meal Prep!**\n",
              "\n",
              "Tired of those mid-week takeout temptations?  The answer might just be staring you in the face: *meal prep!*\n",
              "\n",
              "Take a peek at the picture – doesn't it just make you hungry?  Imagine having these vibrant, wholesome meals ready to grab from your fridge whenever hunger strikes.\n",
              "\n",
              "**Why Meal Prep is Your Secret Weapon:**\n",
              "\n",
              "*   **Eat Healthier:**  You control the ingredients and portions. No more mystery sauces or hidden calories! This image practically screams \"balance\" with lean protein, colorful veggies, and healthy grains.\n",
              "*   **Save Money:**  Less takeout equals more money in your pocket.  Plus, you can buy ingredients in bulk, which is often cheaper.\n",
              "*   **Save Time:**  A few hours of planning and cooking on the weekend frees up precious time during the busy work week.\n",
              "*   **Reduce Stress:**  No more scrambling for dinner ideas at 6 pm.  Just grab and go!\n",
              "\n",
              "**Ready to Get Started?**\n",
              "\n",
              "Think simple: Roast a big batch of chicken, prepare a pot of rice, and roast your favorite vegetables like broccoli and carrots. A simple marinade with soy sauce, ginger and garlic can elevate your protein. Pack everything into individual containers, and you're set!\n",
              "\n",
              "**What are your favorite meal prep recipes? Share them in the comments below! Let's inspire each other to eat well and live our best lives!**\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with open(\"meal.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        "    # Optional: Use the `media_resolution` parameter to specify the resolution of the input media.\n",
        "    config=GenerateContentConfig(\n",
        "        media_resolution=MediaResolution.MEDIA_RESOLUTION_LOW,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Sg1vLkOB4Nb"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/anshupandey/ms-generativeai-apr2025/refs/heads/main/generative-ai_image_covid_chart.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUwy1wLDB4Nb"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/anshupandey/ms-generativeai-apr2025/refs/heads/main/generative-ai_image_covid_chart.png -O generative-ai_image_covid_chart.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT6jTevSB4Nb",
        "outputId": "05bcf550-a2d7-4639-8b56-2c8222ee9db0"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The image displays the percentage of people fully and partially vaccinated against COVID-19 in various countries as of September 2, 2021. China has the highest percentage of fully vaccinated people, while Vietnam has the lowest percentage."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with open(\"generative-ai_image_covid_chart.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Explain this image in 2 lines.\",\n",
        "    ],\n",
        "    # Optional: Use the `media_resolution` parameter to specify the resolution of the input media.\n",
        "    config=GenerateContentConfig(\n",
        "        media_resolution=MediaResolution.MEDIA_RESOLUTION_LOW,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvWxwCOwB4Nc",
        "outputId": "2fc1b28a-021c-4557-e3c4-ac6ed00799d5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Here's the data extracted from the image, presented in a markdown table:\n",
              "\n",
              "| Country       | Share of People Fully Vaccinated Against COVID-19 (%) | Share of People Only Partly Vaccinated Against COVID-19 (%) |\n",
              "|---------------|------------------------------------------------------|--------------------------------------------------------------|\n",
              "| China         | 74%                                                   | 13%                                                          |\n",
              "| Italy         | 71%                                                   | 9.7%                                                         |\n",
              "| Germany       | 65%                                                   | 4.5%                                                          |\n",
              "| United States | 61%                                                   | 9.3%                                                         |\n",
              "| Mexico        | 27%                                                   | 18%                                                         |\n",
              "| Taiwan        | 4%                                                    | 38%                                                         |\n",
              "| India         | 11%                                                   | 26%                                                         |\n",
              "| Thailand      | 11%                                                   | 22%                                                         |\n",
              "| Indonesia     | 13%                                                   | 10%                                                         |\n",
              "| Vietnam       | 2.8%                                                   | 15%                                                         |\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with open(\"generative-ai_image_covid_chart.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Extract the data from the image and present it in markdown table\",\n",
        "    ],\n",
        "    # Optional: Use the `media_resolution` parameter to specify the resolution of the input media.\n",
        "    config=GenerateContentConfig(\n",
        "        media_resolution=MediaResolution.MEDIA_RESOLUTION_LOW,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRQyv1DhTbnH"
      },
      "source": [
        "### Send document from Google Cloud Storage\n",
        "\n",
        "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
        "\n",
        "Check out this notebook for more examples of document understanding with Gemini:\n",
        "\n",
        "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG6l1Fuka6ZJ",
        "outputId": "045a8bc7-a6ef-448c-fcc5-75b69b03d0cc"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Here's a summary of the paper, \"Attention is All You Need\":\n",
              "\n",
              "This paper introduces the Transformer, a novel neural network architecture for sequence transduction tasks that relies solely on attention mechanisms, dispensing with recurrence and convolutions. Experiments on machine translation tasks demonstrate that the Transformer achieves superior quality compared to recurrent or convolutional models while being more parallelizable and requiring significantly less training time.\n",
              "\n",
              "The key highlights include:\n",
              "\n",
              "*   **Attention-Based Architecture:** The Transformer relies entirely on attention mechanisms to model dependencies between input and output sequences.\n",
              "\n",
              "*   **Parallelization:** The architecture enables significant parallelization during training.\n",
              "\n",
              "*   **Superior Translation Quality:** The Transformer achieves state-of-the-art results on WMT 2014 English-to-German and English-to-French translation tasks.\n",
              "\n",
              "*   **Generalization:** The model demonstrates successful application to other tasks such as English constituency parsing.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
        "            mime_type=\"application/pdf\",\n",
        "        ),\n",
        "        \"Summarize the document.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25n22nc6TdZw"
      },
      "source": [
        "### Send audio from General URL\n",
        "\n",
        "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVU9XyCCo-h2",
        "outputId": "12980166-ef17-44c5-afcf-f94dbbbf86ef"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Sure, here is a summary of the podcast episode.\n",
              "\n",
              "In this episode, titled \"KubeCon North America 2024 Coverage,\" hosts discuss the latest news in the Kubernetes ecosystem and feature interviews with attendees at KubeCon. The topics covered include project graduations of CNCF projects, security protocols for Kubernetes, the CNCF’s cloud native hero challenge, the lineup for next year's flagship events, and new cloud native certifications. Kathleen interviews attendees to capture their personal experiences at the event, and their feedback, the reasons they attended the show, and the trends they observed. The guests touched on hot topics like integrating AI with cloud natives, security enhancements for Kubernetes and Istio, as well as the importance of connecting and reconnecting with the Kubernetes community. Overall, the podcast provides a comprehensive overview of recent advancements, events, and community engagement within the Kubernetes and cloud native space."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
        "            mime_type=\"audio/mpeg\",\n",
        "        ),\n",
        "        \"Write a summary of this podcast episode.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(audio_timestamp=True),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D3_oNUTuW2q"
      },
      "source": [
        "### Send video from YouTube URL\n",
        "\n",
        "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7-w8G_2wAOw",
        "outputId": "f18cab68-433e-4e72-b8c4-39d99baf9f0d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Harry Potter is shown in the video at [00:00:57]."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "video = Part.from_uri(\n",
        "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        \"At what point in the video is Harry Potter shown?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df8013cfa7f7"
      },
      "source": [
        "### Send web page\n",
        "\n",
        "This example is from the [Generative AI on Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/overview).\n",
        "\n",
        "**NOTE:** The URL must be publicly accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "337793322c91",
        "outputId": "84aa324a-8545-4294-8b2b-073e093fddf1"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "This Google Cloud documentation provides a comprehensive guide to generative AI on Vertex AI. It covers topics from getting started with API keys and Vertex AI Studio to selecting models from Model Garden and Google Models like Gemini, Imagen, and Veo. It also details how to build and deploy AI agents, design effective prompts, tune models, and evaluate their performance using AI-powered tools. Additionally, it emphasizes enterprise-grade security, access control, networking, and responsible AI practices for generative AI applications."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://cloud.google.com/vertex-ai/generative-ai/docs\",\n",
        "            mime_type=\"text/html\",\n",
        "        ),\n",
        "        \"Write a summary of this documentation.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfe17y5NB_6w"
      },
      "source": [
        "## Multimodal Live API\n",
        "\n",
        "The Multimodal Live API enables low-latency bidirectional voice and video interactions with Gemini. Using the Multimodal Live API, you can provide end users with the experience of natural, human-like voice conversations, and with the ability to interrupt the model's responses using voice commands. The model can process text, audio, and video input, and it can provide text and audio output.\n",
        "\n",
        "The Multimodal Live API is built on [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API).\n",
        "\n",
        "For more examples with the Multimodal Live API, refer to the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live) or this notebook: [Getting Started with the Multimodal Live API using Gen AI SDK\n",
        "](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/intro_multimodal_live_api_genai_sdk.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlo0mWuZGkQ"
      },
      "source": [
        "## Control generated output\n",
        "\n",
        "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
        "\n",
        "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
        "\n",
        "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n",
        "\n",
        "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjSgf2cDN_bG",
        "outputId": "5ccba6d9-b748-49c0-c2f6-c069859e9ae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"name\": \"Chocolate Chip Cookies\",\n",
            "  \"description\": \"Classic cookies with chocolate chips.\",\n",
            "  \"ingredients\": [\"Butter\", \"Sugar\", \"Brown Sugar\", \"Eggs\", \"Vanilla Extract\", \"All-Purpose Flour\", \"Baking Soda\", \"Salt\", \"Chocolate Chips\"]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKai5CP_PGQF"
      },
      "source": [
        "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuVe_ahuB4Ng",
        "outputId": "050dcf14-d04c-45ba-845e-db29e19e358a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GenerateContentResponse(\n",
              "  automatic_function_calling_history=[],\n",
              "  candidates=[\n",
              "    Candidate(\n",
              "      content=Content(\n",
              "        parts=[\n",
              "          Part(\n",
              "            text=\"\"\"The current Vice President of India is C. P. Radhakrishnan, who assumed office on September 12, 2025. He won the election on September 9, 2025, defeating opposition candidate B Sudershan Reddy.\n",
              "\"\"\"\n",
              "          ),\n",
              "        ],\n",
              "        role='model'\n",
              "      ),\n",
              "      finish_reason=<FinishReason.STOP: 'STOP'>,\n",
              "      grounding_metadata=GroundingMetadata(\n",
              "        grounding_chunks=[\n",
              "          GroundingChunk(\n",
              "            web=GroundingChunkWeb(\n",
              "              domain=<... Max depth ...>,\n",
              "              title=<... Max depth ...>,\n",
              "              uri=<... Max depth ...>\n",
              "            )\n",
              "          ),\n",
              "          GroundingChunk(\n",
              "            web=GroundingChunkWeb(\n",
              "              domain=<... Max depth ...>,\n",
              "              title=<... Max depth ...>,\n",
              "              uri=<... Max depth ...>\n",
              "            )\n",
              "          ),\n",
              "          GroundingChunk(\n",
              "            web=GroundingChunkWeb(\n",
              "              domain=<... Max depth ...>,\n",
              "              title=<... Max depth ...>,\n",
              "              uri=<... Max depth ...>\n",
              "            )\n",
              "          ),\n",
              "        ],\n",
              "        grounding_supports=[\n",
              "          GroundingSupport(\n",
              "            confidence_scores=[<... 1 item at Max depth ...>],\n",
              "            grounding_chunk_indices=[<... 1 item at Max depth ...>],\n",
              "            segment=Segment(\n",
              "              end_index=<... Max depth ...>,\n",
              "              start_index=<... Max depth ...>,\n",
              "              text=<... Max depth ...>\n",
              "            )\n",
              "          ),\n",
              "          GroundingSupport(\n",
              "            confidence_scores=[<... 2 items at Max depth ...>],\n",
              "            grounding_chunk_indices=[<... 2 items at Max depth ...>],\n",
              "            segment=Segment(\n",
              "              end_index=<... Max depth ...>,\n",
              "              start_index=<... Max depth ...>,\n",
              "              text=<... Max depth ...>\n",
              "            )\n",
              "          ),\n",
              "        ],\n",
              "        retrieval_metadata=RetrievalMetadata(),\n",
              "        search_entry_point=SearchEntryPoint(\n",
              "          rendered_content=\"\"\"<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHOTcN0mNhkj6_1jOcj2YiSVxEMwiu19fVkPYjRboTdpeiFCtyiCemQCjH_FXtpVP89u7LoLus5m0vPIMgNKwvXkhi1dzYYLAieLe3mYsTgGSPZs-me6la1rSXX0TA0Wo1DCr23wrwoNJrGQuuf3EU_xTeDHw6h_xGhiqlt6bH4trqnJu6PXLwjXl1gVF2ZIknoW-3ddcInGGWHIVp3lw==\">current vice president India</a>\n",
              "  </div>\n",
              "</div>\n",
              "\"\"\"\n",
              "        ),\n",
              "        web_search_queries=[\n",
              "          'current vice president India',\n",
              "        ]\n",
              "      )\n",
              "    ),\n",
              "  ],\n",
              "  create_time=datetime.datetime(2025, 11, 5, 4, 12, 11, 405363, tzinfo=TzInfo(UTC)),\n",
              "  model_version='gemini-2.0-flash',\n",
              "  response_id='m84KafPeGPanrtoPzeDd2AE',\n",
              "  sdk_http_response=HttpResponse(\n",
              "    headers=<dict len=10>\n",
              "  ),\n",
              "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
              "    candidates_token_count=54,\n",
              "    candidates_tokens_details=[\n",
              "      ModalityTokenCount(\n",
              "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
              "        token_count=54\n",
              "      ),\n",
              "    ],\n",
              "    prompt_token_count=9,\n",
              "    prompt_tokens_details=[\n",
              "      ModalityTokenCount(\n",
              "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
              "        token_count=9\n",
              "      ),\n",
              "    ],\n",
              "    total_token_count=63,\n",
              "    traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeyDWbnxO-on",
        "outputId": "b2aa77d2-368c-4aae-8226-4434ba54ad60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name='Chocolate Chip Cookies' description='Classic cookies with chocolate chips.' ingredients=['Butter', 'Sugar', 'Brown Sugar', 'Eggs', 'Vanilla Extract', 'All-Purpose Flour', 'Baking Soda', 'Salt', 'Chocolate Chips']\n"
          ]
        }
      ],
      "source": [
        "parsed_response: Recipe = response.parsed\n",
        "print(parsed_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUSLPrvlvXOc"
      },
      "source": [
        "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
        "\n",
        "- `enum`\n",
        "- `items`\n",
        "- `maxItems`\n",
        "- `nullable`\n",
        "- `properties`\n",
        "- `required`\n",
        "\n",
        "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7duWOq3vMmS",
        "outputId": "953ab6cc-7151-4d70-cc0e-58bc608937c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[{'rating': 4, 'flavor': 'Strawberry Cheesecake', 'sentiment': 'POSITIVE', 'explanation': \"The reviewer expresses strong positive sentiment, stating they 'absolutely loved it' and it's the 'best ice cream' they've had.\"}, {'rating': 1, 'flavor': 'Mango Tango', 'sentiment': 'NEGATIVE', 'explanation': \"Despite acknowledging it's 'quite good', the reviewer finds it 'a bit too sweet', indicating a negative aspect affecting their overall experience, implied by the 1-star rating.\"}]]\n"
          ]
        }
      ],
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"rating\": {\"type\": \"INTEGER\"},\n",
        "                \"flavor\": {\"type\": \"STRING\"},\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"STRING\",\n",
        "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
        "                },\n",
        "                \"explanation\": {\"type\": \"STRING\"},\n",
        "            },\n",
        "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
        "\n",
        "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
        "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "    ),\n",
        ")\n",
        "\n",
        "response_dict = response.parsed\n",
        "print(response_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1dR-QlTKRs"
      },
      "source": [
        "## Count tokens and compute tokens\n",
        "\n",
        "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
        "\n",
        "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syx-fwLkV1j-"
      },
      "source": [
        "### Count tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhNElguLRRNK",
        "outputId": "71ea81b5-c5de-4733-d3fd-2fedc10cc185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sdk_http_response=HttpResponse(\n",
            "  headers=<dict len=10>\n",
            ") total_tokens=9 cached_content_token_count=None\n"
          ]
        }
      ],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS-AP7AHUQmV"
      },
      "source": [
        "### Compute tokens\n",
        "\n",
        "The `compute_tokens()` method runs a local tokenizer instead of making an API call. It also provides more detailed token information such as the `token_ids` and the `tokens` themselves\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>NOTE: This method is only supported in Vertex AI.</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cdhi5AX1TuH0",
        "outputId": "bcba0a0a-aa43-4916-9e1c-4818ef86255e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sdk_http_response=HttpResponse(\n",
            "  headers=<dict len=10>\n",
            ") tokens_info=[TokensInfo(\n",
            "  role='user',\n",
            "  token_ids=[\n",
            "    1841,\n",
            "    235303,\n",
            "    235256,\n",
            "    573,\n",
            "    32514,\n",
            "    <... 6 more items ...>,\n",
            "  ],\n",
            "  tokens=[\n",
            "    b'What',\n",
            "    b\"'\",\n",
            "    b's',\n",
            "    b' the',\n",
            "    b' longest',\n",
            "    <... 6 more items ...>,\n",
            "  ]\n",
            ")]\n"
          ]
        }
      ],
      "source": [
        "response = client.models.compute_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the longest word in the English language?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsP0vXOY7hg"
      },
      "source": [
        "## Search as a tool (Grounding)\n",
        "\n",
        "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
        "\n",
        "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
        "\n",
        "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n",
        "\n",
        "For more examples of Grounding, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_M_4RRBdO_3"
      },
      "source": [
        "### Google Search\n",
        "\n",
        "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results.\n",
        "\n",
        "[Dynamic Retrieval](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#dynamic-retrieval) lets you set a threshold for when grounding is used for model responses. This is useful when the prompt doesn't require an answer grounded in Google Search and the supported models can provide an answer based on their knowledge without grounding. This helps you manage latency, quality, and cost more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeR09J3AZT4U",
        "outputId": "bb9fa57e-2601-4e29-c637-b295667854fb"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The current Vice President of India is C. P. Radhakrishnan, who assumed office on September 12, 2025. He won the election on September 9, 2025, defeating opposition candidate B Sudershan Reddy.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "google_maps_widget_context_token=None grounding_chunks=[GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='wikipedia.org',\n",
            "    title='wikipedia.org',\n",
            "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQG-w-AsIdKV80QCQpwG8l9iUOgPTpXW9yGbZDu2uFNFnkA0Ra2H0M64cvg0lLiGW-cAHf03Az8bx8JDtOlDq5clowuhJo0fuGQfBlhS7uDhzdO5udSnfDLL9WgxyxOyduSEaMhPajI_TaGZNIctf6E='\n",
            "  )\n",
            "), GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='livemint.com',\n",
            "    title='livemint.com',\n",
            "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFxo9cDcCGAoM51s40cuXx62ZQPEOp_IowVowCwZa8Lv2CwNUTrSRYiA_R1I297aid7bDVn6zu398YK-Tc13jyzJQHKmnpEUQu1gAqYGVYaejaUyClYTtofmwbpIqENyaFdATfu04Vqb7ZUPCRE_zRtBY_O9Zrzl5JT6CImINfMNaD8T4ltb78KLV99mk9N7vReXOFpmxI56wW3JUk3yzNADE28UFnt6MR0TnLKmaeV3G8e5qVOErqKOMceAsOdxDNHn-H7Z2R-0C34qffQhOmFzb9dkRpF2PO034BSX2_oEOsO'\n",
            "  )\n",
            "), GroundingChunk(\n",
            "  web=GroundingChunkWeb(\n",
            "    domain='youtube.com',\n",
            "    title='youtube.com',\n",
            "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGjsiKgKYGo1AeXbHda6JYR0Xub-Ux0kDAr9ZQ3EIGK4OxLQi06ZYEkaKLD87FkA7Hi7T0iulQOZZio_LM89IjoajJJEdO0zyBbYF_QKVbgDImJlPHEk_ctlWrby8QrBpnHV2EinA=='\n",
            "  )\n",
            ")] grounding_supports=[GroundingSupport(\n",
            "  confidence_scores=[\n",
            "    0.9164721,\n",
            "  ],\n",
            "  grounding_chunk_indices=[\n",
            "    0,\n",
            "  ],\n",
            "  segment=Segment(\n",
            "    end_index=100,\n",
            "    start_index=45,\n",
            "    text='Radhakrishnan, who assumed office on September 12, 2025'\n",
            "  )\n",
            "), GroundingSupport(\n",
            "  confidence_scores=[\n",
            "    0.6081881,\n",
            "    0.5129579,\n",
            "  ],\n",
            "  grounding_chunk_indices=[\n",
            "    1,\n",
            "    2,\n",
            "  ],\n",
            "  segment=Segment(\n",
            "    end_index=192,\n",
            "    start_index=102,\n",
            "    text='He won the election on September 9, 2025, defeating opposition candidate B Sudershan Reddy'\n",
            "  )\n",
            ")] retrieval_metadata=RetrievalMetadata() retrieval_queries=None search_entry_point=SearchEntryPoint(\n",
            "  rendered_content=\"\"\"<style>\n",
            ".container {\n",
            "  align-items: center;\n",
            "  border-radius: 8px;\n",
            "  display: flex;\n",
            "  font-family: Google Sans, Roboto, sans-serif;\n",
            "  font-size: 14px;\n",
            "  line-height: 20px;\n",
            "  padding: 8px 12px;\n",
            "}\n",
            ".chip {\n",
            "  display: inline-block;\n",
            "  border: solid 1px;\n",
            "  border-radius: 16px;\n",
            "  min-width: 14px;\n",
            "  padding: 5px 16px;\n",
            "  text-align: center;\n",
            "  user-select: none;\n",
            "  margin: 0 8px;\n",
            "  -webkit-tap-highlight-color: transparent;\n",
            "}\n",
            ".carousel {\n",
            "  overflow: auto;\n",
            "  scrollbar-width: none;\n",
            "  white-space: nowrap;\n",
            "  margin-right: -12px;\n",
            "}\n",
            ".headline {\n",
            "  display: flex;\n",
            "  margin-right: 4px;\n",
            "}\n",
            ".gradient-container {\n",
            "  position: relative;\n",
            "}\n",
            ".gradient {\n",
            "  position: absolute;\n",
            "  transform: translate(3px, -9px);\n",
            "  height: 36px;\n",
            "  width: 9px;\n",
            "}\n",
            "@media (prefers-color-scheme: light) {\n",
            "  .container {\n",
            "    background-color: #fafafa;\n",
            "    box-shadow: 0 0 0 1px #0000000f;\n",
            "  }\n",
            "  .headline-label {\n",
            "    color: #1f1f1f;\n",
            "  }\n",
            "  .chip {\n",
            "    background-color: #ffffff;\n",
            "    border-color: #d2d2d2;\n",
            "    color: #5e5e5e;\n",
            "    text-decoration: none;\n",
            "  }\n",
            "  .chip:hover {\n",
            "    background-color: #f2f2f2;\n",
            "  }\n",
            "  .chip:focus {\n",
            "    background-color: #f2f2f2;\n",
            "  }\n",
            "  .chip:active {\n",
            "    background-color: #d8d8d8;\n",
            "    border-color: #b6b6b6;\n",
            "  }\n",
            "  .logo-dark {\n",
            "    display: none;\n",
            "  }\n",
            "  .gradient {\n",
            "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
            "  }\n",
            "}\n",
            "@media (prefers-color-scheme: dark) {\n",
            "  .container {\n",
            "    background-color: #1f1f1f;\n",
            "    box-shadow: 0 0 0 1px #ffffff26;\n",
            "  }\n",
            "  .headline-label {\n",
            "    color: #fff;\n",
            "  }\n",
            "  .chip {\n",
            "    background-color: #2c2c2c;\n",
            "    border-color: #3c4043;\n",
            "    color: #fff;\n",
            "    text-decoration: none;\n",
            "  }\n",
            "  .chip:hover {\n",
            "    background-color: #353536;\n",
            "  }\n",
            "  .chip:focus {\n",
            "    background-color: #353536;\n",
            "  }\n",
            "  .chip:active {\n",
            "    background-color: #464849;\n",
            "    border-color: #53575b;\n",
            "  }\n",
            "  .logo-light {\n",
            "    display: none;\n",
            "  }\n",
            "  .gradient {\n",
            "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
            "  }\n",
            "}\n",
            "</style>\n",
            "<div class=\"container\">\n",
            "  <div class=\"headline\">\n",
            "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
            "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
            "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
            "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
            "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
            "    </svg>\n",
            "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
            "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
            "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
            "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
            "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
            "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
            "    </svg>\n",
            "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
            "  </div>\n",
            "  <div class=\"carousel\">\n",
            "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHOTcN0mNhkj6_1jOcj2YiSVxEMwiu19fVkPYjRboTdpeiFCtyiCemQCjH_FXtpVP89u7LoLus5m0vPIMgNKwvXkhi1dzYYLAieLe3mYsTgGSPZs-me6la1rSXX0TA0Wo1DCr23wrwoNJrGQuuf3EU_xTeDHw6h_xGhiqlt6bH4trqnJu6PXLwjXl1gVF2ZIknoW-3ddcInGGWHIVp3lw==\">current vice president India</a>\n",
            "  </div>\n",
            "</div>\n",
            "\"\"\"\n",
            ") source_flagging_uris=None web_search_queries=['current vice president India']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHOTcN0mNhkj6_1jOcj2YiSVxEMwiu19fVkPYjRboTdpeiFCtyiCemQCjH_FXtpVP89u7LoLus5m0vPIMgNKwvXkhi1dzYYLAieLe3mYsTgGSPZs-me6la1rSXX0TA0Wo1DCr23wrwoNJrGQuuf3EU_xTeDHw6h_xGhiqlt6bH4trqnJu6PXLwjXl1gVF2ZIknoW-3ddcInGGWHIVp3lw==\">current vice president India</a>\n",
              "  </div>\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "google_search_tool = Tool(google_search=GoogleSearch())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Who is the current vice president of India?\",\n",
        "    config=GenerateContentConfig(tools=[google_search_tool]),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "\n",
        "print(response.candidates[0].grounding_metadata)\n",
        "\n",
        "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0pb-Kh1xEHU"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
        "\n",
        "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
        "\n",
        "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "For more examples of Function calling with Gemini, check out this notebook: [Intro to Function Calling with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSUWWlrrlR-D"
      },
      "source": [
        "### Python Function (Automatic Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRR8HZhLlR-E",
        "outputId": "9dd44db3-61ec-4835-c28e-c452a03cd8d1"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "It is hot in Austin, TX.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Example method. Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    weather_map: dict[str, str] = {\n",
        "        \"Boston, MA\": \"snowing\",\n",
        "        \"San Francisco, CA\": \"foggy\",\n",
        "        \"Seattle, WA\": \"raining\",\n",
        "        \"Austin, TX\": \"hot\",\n",
        "        \"Chicago, IL\": \"windy\",\n",
        "    }\n",
        "    return weather_map.get(location, \"unknown\")\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the weather like in Austin?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4syyLEClGcn"
      },
      "source": [
        "### OpenAPI Specification (Manual Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BDQPwgcxRN3",
        "outputId": "494a0093-aaa4-4bdc-af08-bf173f9e5e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id=None args={'destination': 'Paris'} name='get_destination'\n"
          ]
        }
      ],
      "source": [
        "get_destination = FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.function_calls[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhDs2X3o0neK"
      },
      "source": [
        "## Code Execution\n",
        "\n",
        "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
        "\n",
        "The Gemini API provides code execution as a tool, similar to function calling.\n",
        "After you add code execution as a tool, the model decides when to use it.\n",
        "\n",
        "For more examples of Code Execution, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W-3c7sy0nyz",
        "outputId": "99f6dcf9-04a8-4e54-e507-f21252f6321a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\n",
              "## Code\n",
              "\n",
              "```py\n",
              "def fibonacci(n):\n",
              "    if n <= 0:\n",
              "        return 0\n",
              "    elif n == 1:\n",
              "        return 1\n",
              "    else:\n",
              "        a, b = 0, 1\n",
              "        for _ in range(2, n + 1):\n",
              "            a, b = b, a + b\n",
              "        return b\n",
              "\n",
              "fib_20 = fibonacci(20)\n",
              "print(f'{fib_20=}')\n",
              "\n",
              "```\n",
              "\n",
              "### Output\n",
              "\n",
              "```\n",
              "fib_20=6765\n",
              "\n",
              "```\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[code_execution_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"\n",
        "## Code\n",
        "\n",
        "```py\n",
        "{response.executable_code}\n",
        "```\n",
        "\n",
        "### Output\n",
        "\n",
        "```\n",
        "{response.code_execution_result}\n",
        "```\n",
        "\"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsdayvxmB4Nl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_gemini_2_0_flash.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}