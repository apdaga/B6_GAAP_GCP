{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bluedata-Consulting/GAAPB01-training-code-base/blob/main/SELFLEARNING_LangChain_Chains_Routing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031a0acd",
      "metadata": {},
      "source": [
        "# Chains - Routing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b47436a",
      "metadata": {
        "id": "4b47436a"
      },
      "source": [
        "# How to route between sub-chains\n",
        "\n",
        ":::info Prerequisites\n",
        "\n",
        "This guide assumes familiarity with the following concepts:\n",
        "- [LangChain Expression Language (LCEL)](/docs/concepts/#langchain-expression-language)\n",
        "- [Chaining runnables](/docs/how_to/sequence/)\n",
        "- [Configuring chain parameters at runtime](/docs/how_to/configure)\n",
        "- [Prompt templates](/docs/concepts/#prompt-templates)\n",
        "- [Chat Messages](/docs/concepts/#message-types)\n",
        "\n",
        ":::\n",
        "\n",
        "Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing can help provide structure and consistency around interactions with models by allowing you to define states and use information related to those states as context to model calls.\n",
        "\n",
        "There are two ways to perform routing:\n",
        "\n",
        "1. Conditionally return runnables from a [`RunnableLambda`](/docs/how_to/functions) (recommended)\n",
        "2. Using a `RunnableBranch` (legacy)\n",
        "\n",
        "We'll illustrate both methods using a two step sequence where the first step classifies an input question as being about `LangChain`, `Anthropic`, or `Other`, then routes to a corresponding prompt chain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c6edac",
      "metadata": {
        "id": "c1c6edac"
      },
      "source": [
        "## Example Setup\n",
        "First, let's create a chain that will identify incoming questions as being about `LangChain`, `Anthropic`, or `Other`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e7be8062",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name='gemini-2.0-flash'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8a8a1967",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8a8a1967",
        "outputId": "13bbe756-f42c-45dc-b36c-ca89e7c61d69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Anshu Pandey\\AppData\\Local\\Temp\\ipykernel_18272\\3127084504.py:6: LangChainBetaWarning: The function `init_chat_model` is in beta. It is actively being worked on, so the API may change.\n",
            "  model = init_chat_model(model_name, model_provider=\"google_genai\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Gemini\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "model = init_chat_model(model_name, model_provider=\"google_genai\")\n",
        "\n",
        "chain = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"Given the user question below, classify it as either being about `LangChain`, `Gemini`, or `Other`.\n",
        "\n",
        "Do not respond with more than one word.\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "Classification:\"\"\"\n",
        "    )\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke({\"question\": \"how do I call Gemini?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "rhpM0hSvPe4J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rhpM0hSvPe4J",
        "outputId": "e9a5d8d3-fa84-449d-d550-0466c3462df6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'LangChain\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"how do I use LangChain?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "KjaNGHrGPivP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "KjaNGHrGPivP",
        "outputId": "7c01fff5-4ba5-4019-c91c-97656cd2771d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Other\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"what are the models available with OpenAI API?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7655555f",
      "metadata": {
        "id": "7655555f"
      },
      "source": [
        "Now, let's create three sub chains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "89d7722d",
      "metadata": {
        "id": "89d7722d"
      },
      "outputs": [],
      "source": [
        "langchain_chain = PromptTemplate.from_template(\n",
        "    \"\"\"You are an expert in langchain. \\\n",
        "Always answer questions starting with \"As Harrison Chase told me\". \\\n",
        "Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | model\n",
        "\n",
        "\n",
        "\n",
        "gemini_chain = PromptTemplate.from_template(\n",
        "    \"\"\"You are an expert in Vertex AI Gemini Models. \\\n",
        "Always answer questions starting with \"As Sundar Pichai told me\". \\\n",
        "Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | model\n",
        "\n",
        "\n",
        "general_chain = PromptTemplate.from_template(\n",
        "    \"\"\"Respond to the following question:\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ") | model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d8d042c",
      "metadata": {
        "id": "6d8d042c"
      },
      "source": [
        "## Using a custom function (Recommended)\n",
        "\n",
        "You can also use a custom function to route between different outputs. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "687492da",
      "metadata": {
        "id": "687492da"
      },
      "outputs": [],
      "source": [
        "def route(info):\n",
        "    if \"gemini\" in info[\"topic\"].lower():\n",
        "        return gemini_chain\n",
        "    elif \"langchain\" in info[\"topic\"].lower():\n",
        "        return langchain_chain\n",
        "    else:\n",
        "        return general_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "02a33c86",
      "metadata": {
        "id": "02a33c86"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(\n",
        "    route\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c2e977a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2e977a4",
        "outputId": "83c00a3b-e0a9-4297-eff2-4d7182117a63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"As Sundar Pichai told me, you can use Gemini through various channels, depending on your needs and technical expertise:\\n\\n*   **Google AI Studio (Maker Suite):** This is a web-based interface designed for prototyping and experimenting with Gemini. It's perfect for beginners and allows you to quickly test different prompts and parameters without writing any code. You can access it at [https://makersuite.google.com/app/home](https://makersuite.google.com/app/home).\\n\\n*   **Vertex AI (Developer Suite):** For more advanced users and production deployments, Vertex AI offers a robust platform. It provides the following:\\n    *   **API Access:** You can use the Gemini API to integrate the model into your applications. This requires coding in languages like Python, Node.js, Java, etc., using the Google Cloud Client Libraries.\\n    *   **Vertex AI Workbench:** A managed Jupyter notebook environment for data exploration, model development, and experimentation.\\n    *   **Model Garden:** A catalog of pre-trained models (including Gemini) that you can deploy and fine-tune.\\n    *   **Vertex AI Pipelines:** For building and managing machine learning workflows.\\n\\n*   **Google Cloud Console:** If you're already familiar with Google Cloud, you can manage Gemini models and resources directly through the Google Cloud Console.\\n\\n**Here's a simplified breakdown of the steps, depending on your chosen method:**\\n\\n*   **Using Google AI Studio:**\\n    1.  Go to [https://makersuite.google.com/app/home](https://makersuite.google.com/app/home)\\n    2.  Sign in with your Google account.\\n    3.  Select a prompt template or start with a blank prompt.\\n    4.  Write your prompt and adjust the parameters (e.g., temperature, max output tokens).\\n    5.  Run the prompt and iterate until you get the desired results.\\n\\n*   **Using Vertex AI (API):**\\n    1.  Set up a Google Cloud project and enable the Vertex AI API.\\n    2.  Install the Google Cloud Client Libraries for your preferred programming language.\\n    3.  Authenticate your application to access the API.\\n    4.  Write code to send requests to the Gemini API, specifying your prompt and parameters.\\n    5.  Process the response from the API.\\n\\n**Key Considerations:**\\n\\n*   **Cost:** Be aware of the pricing for using Gemini, especially when using the Vertex AI API. Google Cloud offers free tiers for experimentation, but usage beyond that will incur charges.\\n*   **Quotas:** Google Cloud enforces quotas on API usage. You may need to request quota increases if you plan to use Gemini extensively.\\n*   **Prompt Engineering:** The quality of your prompts is crucial for getting good results. Experiment with different phrasing, instructions, and examples to optimize your prompts.\\n*   **Safety:** Gemini has safety filters in place to prevent harmful or inappropriate content. Be mindful of these filters when crafting your prompts.\\n\\nIn short, start with Google AI Studio if you want a quick and easy way to experiment. Use Vertex AI if you need more control, scalability, and integration with other Google Cloud services. Good luck!\\n\", response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-71cf9bf5-fac0-420d-a7eb-87d59eab0d9e-0', usage_metadata={'input_tokens': 42, 'output_tokens': 684, 'total_tokens': 726})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"how do I use Gemini?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "48913dc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48913dc6",
        "outputId": "b6d54820-821b-4d28-e833-a7a2c4666d19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='As Harrison Chase told me, using LangChain is all about leveraging its modular components to build sophisticated language applications. Here\\'s a breakdown of how you can approach it:\\n\\n**1. Understanding the Core Modules:**\\n\\nLangChain is built around several key modules. You\\'ll want to familiarize yourself with these:\\n\\n*   **Models:** This is where you interface with different language models (LLMs) like OpenAI\\'s GPT models, Cohere, Hugging Face models, and more. You can interact with them for text generation, summarization, translation, etc.\\n*   **Prompts:** Prompts are the instructions you give to the language model. LangChain helps you manage and optimize prompts using PromptTemplates, Example Selectors, and Output Parsers.  Effective prompting is crucial for getting the desired output.\\n*   **Chains:** Chains are sequences of calls to LLMs or other utilities. They link together different components to create more complex applications. Common chains include LLMChain (for simple LLM applications) and Sequential Chains (for running multiple chains in a sequence).\\n*   **Indexes (Document Loaders and Vectorstores):**  This module deals with loading, transforming, and storing documents. Document Loaders allow you to ingest data from various sources (text files, PDFs, websites, databases).  Vectorstores are used to embed and store document chunks for efficient retrieval (often used with embeddings). Vectorstores help you create a knowledge base for your LLM to draw upon.\\n*   **Memory:** LLMs are stateless by default. The Memory module allows you to add state to your applications, remembering past interactions and incorporating them into future prompts.  This is essential for conversational applications.\\n*   **Agents:** Agents use an LLM to determine which actions to take. They interact with tools (like search engines, calculators, databases) to gather information and make decisions. Agents are useful for tasks that require reasoning and exploration.\\n*   **Callbacks:** Callbacks are used to log and monitor the execution of LangChain components.  They can be helpful for debugging, tracking usage, and visualizing the flow of data through your application.\\n\\n**2. Installation:**\\n\\nStart by installing LangChain using pip:\\n\\n```bash\\npip install langchain\\n```\\n\\nYou\\'ll also likely need to install specific integrations depending on the LLMs and tools you want to use (e.g., `pip install openai`, `pip install chromadb`).\\n\\n**3. Basic Usage (Example: LLMChain):**\\n\\nLet\\'s illustrate with a simple example using an LLMChain:\\n\\n```python\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n\\n# 1. Initialize the LLM\\nllm = OpenAI(temperature=0.7, openai_api_key=\"YOUR_OPENAI_API_KEY\")  # Replace with your API key\\n\\n# 2. Create a PromptTemplate\\nprompt_template = PromptTemplate(\\n    input_variables=[\"topic\"],\\n    template=\"Write a short story about: {topic}\"\\n)\\n\\n# 3. Construct the LLMChain\\nchain = LLMChain(llm=llm, prompt=prompt_template)\\n\\n# 4. Run the Chain\\nstory = chain.run(\"a magical cat who can talk\")\\n\\nprint(story)\\n```\\n\\n**Explanation:**\\n\\n*   We initialize an OpenAI LLM.  Make sure you have an OpenAI API key and set it.\\n*   We define a PromptTemplate that takes a `topic` as input and generates a prompt for the LLM.\\n*   We create an LLMChain, linking the LLM and the prompt template.\\n*   We call `chain.run()` with the topic \"a magical cat who can talk\", and it returns the generated story.\\n\\n**4. Building More Complex Applications:**\\n\\n*   **Chains:**  For more complex workflows, explore Sequential Chains (SimpleSequentialChain, SequentialChain) to chain multiple chains together. Consider using Router Chains to dynamically select the next chain to execute based on the input.\\n*   **Document Question Answering:**  Combine Document Loaders, Text Splitters, Embeddings, Vectorstores, and RetrievalQA Chains to answer questions based on your own documents.\\n*   **Conversational Applications:** Use the Memory module (ConversationBufferMemory, ConversationBufferWindowMemory, etc.) to maintain conversation history.  Combine this with chains to create chatbots.\\n*   **Agents:**  For tasks that require reasoning and decision-making, use Agents with tools.  LangChain provides pre-built tools (search, calculator, etc.) and allows you to create your own.\\n\\n**5. Key Considerations:**\\n\\n*   **API Keys:** Remember to handle your API keys securely.  Don\\'t hardcode them directly into your code. Use environment variables or secrets management tools.\\n*   **Prompt Engineering:** Experiment with different prompts to optimize the LLM\\'s output. Consider using few-shot learning (providing examples in the prompt).\\n*   **Cost:** Be mindful of the cost of using LLMs, especially when using API-based models. Monitor your usage and set limits if necessary.\\n*   **Error Handling:** Implement proper error handling to catch exceptions and prevent your application from crashing.\\n*   **Security:** When building applications that handle user input, be aware of potential security risks (prompt injection, etc.). Sanitize user input and validate the LLM\\'s output.\\n*   **Text Splitters:** Choosing the right text splitter is crucial when working with long documents. Consider `RecursiveCharacterTextSplitter`, `CharacterTextSplitter`, or `TokenTextSplitter` based on your document structure.\\n\\n**6. Learning Resources:**\\n\\n*   **LangChain Documentation:** The official LangChain documentation is your best resource: [https://python.langchain.com/](https://python.langchain.com/)\\n*   **LangChain Tutorials:** There are many tutorials and examples available online, including the LangChain Cookbook.\\n*   **LangChain Hub:** Explore the LangChain Hub for pre-built prompts, chains, and agents.\\n\\nIn summary, LangChain provides a powerful framework for building language applications. Start with the core modules, experiment with different combinations, and consult the documentation to create your own innovative solutions. Good luck!\\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-b8b7dcd2-bcb3-4c0c-b736-d2f9b0a01c4b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 1333, 'total_tokens': 1372})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"how do I use LangChain?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a14d0dca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a14d0dca",
        "outputId": "c5f2fff8-00c9-41b1-8293-39e5d3cc0719"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='4\\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-d5a97293-4f5a-4971-9190-af9327459840-0', usage_metadata={'input_tokens': 18, 'output_tokens': 2, 'total_tokens': 20})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5147b827",
      "metadata": {
        "id": "5147b827"
      },
      "source": [
        "## Using a RunnableBranch\n",
        "\n",
        "A `RunnableBranch` is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input. It does **not** offer anything that you can't achieve in a custom function as described above, so we recommend using a custom function instead.\n",
        "\n",
        "A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.\n",
        "\n",
        "If no provided conditions match, it runs the default runnable.\n",
        "\n",
        "Here's an example of what it looks like in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2a101418",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a101418",
        "outputId": "eb836e77-3875-4b70-fcf8-61db67f31701"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"As Sundar Pichai told me, you can use Gemini in a few key ways:\\n\\n*   **Through the Vertex AI API:** This is for developers and businesses who want to integrate Gemini's capabilities directly into their applications, workflows, and products. You'll be able to access the models programmatically and customize them for specific tasks. You'll need a Google Cloud project and familiarity with coding to use this method.\\n\\n*   **Through the Google AI Studio:** This is a web-based interface designed for prototyping and experimenting with Gemini. It's a great way to get hands-on experience with the model, test different prompts, and explore its capabilities without writing any code. It's more user-friendly for those who aren't developers.\\n\\n*   **Through Google Products:** Gemini is being integrated into various Google products like Search, Workspace (Gmail, Docs, Sheets, Slides), and more. This means you'll be able to leverage Gemini's AI power directly within the tools you already use every day. Keep an eye out for announcements and updates as these features roll out.\\n\\nIn short, choose the method that best suits your needs and technical expertise. If you're a developer wanting to build custom AI-powered applications, use the Vertex AI API. If you want to experiment and explore Gemini's capabilities in a user-friendly environment, use Google AI Studio. And if you're a general user, look for Gemini integrations in your favorite Google products.\\n\", response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-f1ea311e-65f1-475e-a2e7-32345f38df0f-0', usage_metadata={'input_tokens': 42, 'output_tokens': 306, 'total_tokens': 348})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnableBranch\n",
        "\n",
        "branch = RunnableBranch(\n",
        "    (lambda x: \"gemini\" in x[\"topic\"].lower(), gemini_chain),\n",
        "    (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),\n",
        "    general_chain,\n",
        ")\n",
        "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branch\n",
        "full_chain.invoke({\"question\": \"how do I use Gemini?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8d8caf9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d8caf9b",
        "outputId": "03f64370-3c05-4adb-c463-27101833fa2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='As Harrison Chase told me, using LangChain involves a few key steps:\\n\\n1. **Installation:** First, you\\'ll need to install the LangChain Python package. You can do this using pip: `pip install langchain`. You might also need to install other packages depending on the specific integrations you plan to use (e.g., `pip install openai` for OpenAI models).\\n\\n2. **Choosing a Model:** LangChain is all about working with language models. You\\'ll need to select a language model provider (like OpenAI, Cohere, Hugging Face, or even a local model). Then, you\\'ll instantiate the model object in LangChain. For example, using OpenAI:\\n\\n   ```python\\n   from langchain.llms import OpenAI\\n\\n   llm = OpenAI(openai_api_key=\"YOUR_API_KEY\") # Replace with your actual API key\\n   ```\\n\\n3. **Chains:** The core concept in LangChain is the \"Chain\". Chains are sequences of calls to language models or other utilities. The simplest chain just passes your input directly to the language model and returns the output.  However, chains can get much more complex.\\n\\n   *   **LLMChain:** This is a fundamental chain that takes an LLM and some prompts. You can use it like this:\\n\\n       ```python\\n       from langchain.chains import LLMChain\\n       from langchain.prompts import PromptTemplate\\n\\n       prompt = PromptTemplate(\\n           input_variables=[\"product\"],\\n           template=\"What is a good name for a company that makes {product}?\",\\n       )\\n\\n       chain = LLMChain(llm=llm, prompt=prompt)\\n\\n       print(chain.run(\"colorful socks\"))\\n       ```\\n\\n   *   **Sequential Chains:** These let you chain multiple chains together. You can have a SimpleSequentialChain (where the output of one chain is directly the input to the next) or a SequentialChain (which allows you to specify the input and output keys for each chain).\\n\\n4. **Prompts:** Prompts are crucial. They\\'re the instructions you give to the language model. LangChain provides a `PromptTemplate` class to help you create reusable prompts with input variables.  Spend time crafting effective prompts to get the best results.\\n\\n5. **Memory:** If you want your LLM to remember previous interactions, you\\'ll need to use memory. LangChain offers different types of memory, like `ConversationBufferMemory` (which stores the entire conversation), `ConversationBufferWindowMemory` (which stores a limited number of turns), and more sophisticated options.\\n\\n   ```python\\n   from langchain.chains.conversation.memory import ConversationBufferMemory\\n\\n   memory = ConversationBufferMemory()\\n\\n   chain = LLMChain(llm=llm, prompt=prompt, memory=memory) # Assuming you\\'ve defined \\'prompt\\'\\n\\n   print(chain.run(\"My name is John.\"))\\n   print(chain.run(\"What did I just say my name was?\"))\\n   ```\\n\\n6. **Agents:** Agents are where things get really powerful. An agent uses a language model to decide which *tools* to use to answer a question. Tools can be anything from a search engine to a calculator to a database.  LangChain provides several built-in tools and allows you to create your own.\\n\\n   ```python\\n   from langchain.agents import load_tools\\n   from langchain.agents import initialize_agent\\n   from langchain.llms import OpenAI\\n\\n   llm = OpenAI(temperature=0)\\n   tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm) # You might need to install serpapi: pip install serpapi\\n\\n   agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\\n\\n   print(agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"))\\n   ```\\n\\n7. **Document Loaders and Indexing:**  LangChain can load documents from various sources (PDFs, websites, etc.) and index them for retrieval. This is essential for question answering over your own data.  You\\'ll typically use a `DocumentLoader` to load the data, then split the documents into chunks using a `TextSplitter`, and then store the embeddings in a `VectorStore` like Chroma or FAISS.\\n\\n8. **Callbacks:** LangChain has a callback system that allows you to hook into different stages of the chain execution. This is useful for logging, monitoring, and debugging.\\n\\n**Key Takeaways:**\\n\\n*   **Modularity:** LangChain is designed to be modular. You can swap out different components (models, prompts, memory, tools) to customize your application.\\n*   **Flexibility:** There are many ways to accomplish the same task with LangChain. Experiment and find what works best for you.\\n*   **Documentation:** The LangChain documentation is your best friend.  Refer to it frequently as you explore different features and integrations: [https://python.langchain.com/](https://python.langchain.com/)\\n*   **Start Simple:** Begin with basic chains and gradually add complexity as you become more comfortable with the framework.\\n\\nRemember to replace `\"YOUR_API_KEY\"` with your actual OpenAI API key (or the appropriate credentials for your chosen model provider). Also, you might need to install additional packages depending on the specific tools and integrations you use.\\n\\nGood luck!\\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-950192c2-650c-48e3-bb8d-834641096ba6-0', usage_metadata={'input_tokens': 39, 'output_tokens': 1176, 'total_tokens': 1215})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"how do I use LangChain?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "26159af7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26159af7",
        "outputId": "47b7e252-3a52-4bd7-e856-f912203e9d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='4\\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-13925d1c-f4f2-4f1c-9482-c2a16e7894f7-0', usage_metadata={'input_tokens': 18, 'output_tokens': 2, 'total_tokens': 20})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94a706e6",
      "metadata": {},
      "source": [
        "# Using Langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2deb534a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Define the state using Pydantic\n",
        "class QuestionState(TypedDict):\n",
        "    question: str\n",
        "    topic: str\n",
        "    answer: str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ef459ac4",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "def classify_topic(state: QuestionState) -> QuestionState:\n",
        "    topic = chain.invoke({\"question\": state['question']})\n",
        "    state.topic = topic\n",
        "    return {\"topic\":topic}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a26a6982",
      "metadata": {},
      "outputs": [],
      "source": [
        "def route_graph(state: QuestionState) -> str:\n",
        "    topic = (state.topic or \"\").lower()\n",
        "    if \"gemini\" in topic:\n",
        "        return \"gemini\"\n",
        "    elif \"langchain\" in topic:\n",
        "        return \"langchain\"\n",
        "    else:\n",
        "        return \"general\"\n",
        "\n",
        "def run_langchain(state: QuestionState) -> QuestionState:\n",
        "    answer = langchain_chain.invoke({\"question\": state.question})\n",
        "    return {\"answer\":answer}\n",
        "\n",
        "def run_gemini(state: QuestionState) -> QuestionState:\n",
        "    answer = gemini_chain.invoke({\"question\": state.question})\n",
        "    return {\"answer\":answer}\n",
        "\n",
        "def run_general(state: QuestionState) -> QuestionState:\n",
        "    answer = general_chain.invoke({\"question\": state.question})\n",
        "    return {\"answer\":answer}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8babb15c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import START, StateGraph, END\n",
        "\n",
        "graph = StateGraph(QuestionState)\n",
        "graph.add_node(\"classify\", classify_topic)\n",
        "graph.add_node(\"langchain\", run_langchain)\n",
        "graph.add_node(\"gemini\", run_gemini)\n",
        "graph.add_node(\"general\", run_general)\n",
        "\n",
        "graph.add_conditional_edges(\"classify\", route_graph,{\"gemini\":\"gemini\",\"general\":\"general\",\"langchain\":'langchain'})\n",
        "graph.add_edge(START,\"classify\")\n",
        "graph.add_edge(\"general\",END)\n",
        "graph.add_edge(\"langchain\",END)\n",
        "graph.add_edge(\"gemini\",END)\n",
        "\n",
        "chain = graph.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "c767d2a9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(chain.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "607c7a41",
      "metadata": {},
      "outputs": [],
      "source": [
        "for step in chain.stream(\n",
        "    {\"question\": \"How many employees are there?\"}, stream_mode=\"updates\"\n",
        "):\n",
        "    print(step)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff40bcb3",
      "metadata": {
        "id": "ff40bcb3"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "You've now learned how to add routing to your composed LCEL chains.\n",
        "\n",
        "Next, check out the other how-to guides on runnables in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927b7498",
      "metadata": {
        "id": "927b7498"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
