{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Evaluation & Optimization with Vertex AI SDK\n",
        "\n",
        "# ----------------------------------------------------\n",
        "\n",
        "This notebook demonstrates how to perform prompt evaluation and optimization\n",
        "using Vertex AI Generative AI Evaluation SDK.\n",
        "It includes: dataset prep, metric setup, evaluation run, optimization loop."
      ],
      "metadata": {
        "id": "Dz2inzZJcEYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and installation\n"
      ],
      "metadata": {
        "id": "xJURqsgycOq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-cloud-aiplatform[evaluation] --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja4Ebj9AcQbV",
        "outputId": "eca80397-34b5-45c7-d4dc-de07a591da06"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m684.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.1/753.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.cloud import aiplatform\n",
        "import vertexai\n",
        "from vertexai.evaluation import EvalTask, PointwiseMetric, PointwiseMetricPromptTemplate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42cXhSzacT9_",
        "outputId": "85380050-ffcb-4788-8979-54b21be30e13"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
            "  from google.cloud.aiplatform.utils import gcs_utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"your-gcp-project-id\"\n",
        "LOCATION = \"us-central1\"\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "print(f\"Initialized Vertex AI for project {PROJECT_ID}, location {LOCATION}\")"
      ],
      "metadata": {
        "id": "8yGTTG9icX7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare evaluation dataset\n"
      ],
      "metadata": {
        "id": "O4-4LDXTccDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    'prompt': [\"Summarize the benefits of AI in healthcare.\", \"Explain cloud computing in simple terms.\"],\n",
        "    'response': [\"AI improves diagnostics and personalized care.\", \"Cloud computing means using remote servers for storage and processing.\"],\n",
        "    'reference': [\"AI in healthcare aids diagnosis and treatment personalization.\", \"Cloud computing uses internet-based servers instead of local ones.\"]\n",
        "})\n",
        "\n",
        "df.to_json(\"eval_dataset.jsonl\", orient=\"records\", lines=True)\n",
        "print(\"Evaluation dataset created and saved as eval_dataset.jsonl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13RuesHccdjk",
        "outputId": "ef75af59-27bf-4253-a41d-bbbf8322e6fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation dataset created and saved as eval_dataset.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define evaluation metrics\n"
      ],
      "metadata": {
        "id": "JyrlC56icg0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "readability_metric = PointwiseMetric(\n",
        "    metric=\"readability_grade_level\",\n",
        "    metric_prompt_template=PointwiseMetricPromptTemplate(\n",
        "        criteria={\n",
        "            \"grade_level\": (\n",
        "                \"Estimate the U.S. grade-level readability of the response. Lower is simpler, higher is complex.\"\n",
        "            ),\n",
        "            \"conciseness\": (\n",
        "                \"Evaluate how concise the response is: avoids unnecessary words while preserving meaning.\"\n",
        "            )\n",
        "        },\n",
        "        rating_rubric={\n",
        "            \"1\": \"Excellent readability and conciseness.\",\n",
        "            \"0\": \"Average readability and conciseness.\",\n",
        "            \"-1\": \"Poor readability or verbosity.\"\n",
        "        },\n",
        "        input_variables=[\"prompt\", \"response\", \"reference\"]\n",
        "    )\n",
        ")\n",
        "print(\"Custom readability metric defined.\")\n"
      ],
      "metadata": {
        "id": "ukEq07aLclon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run evaluation"
      ],
      "metadata": {
        "id": "xlR8NodacoDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_task = EvalTask(\n",
        "    dataset=\"eval_dataset.jsonl\",\n",
        "    metrics=[readability_metric, \"bleu\", \"rouge_l_sum\"],\n",
        "    experiment=\"prompt_eval_experiment\"\n",
        ")\n",
        "\n",
        "print(\"Running evaluation task...\")\n",
        "eval_result = eval_task.evaluate(\n",
        "    model=\"projects/your-project/locations/us-central1/models/YOUR_MODEL_ID\",\n",
        "    prompt_template=\"{prompt}\"\n",
        ")\n",
        "print(\"Evaluation completed.\")\n"
      ],
      "metadata": {
        "id": "CeXKOY7_crD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View evaluation results\n"
      ],
      "metadata": {
        "id": "I8NbbsR4ctlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(eval_result.metrics_summary)\n",
        "per_instance = eval_result.to_dataframe()\n",
        "per_instance.sort_values(by=\"readability_grade_level\", ascending=True).head(5)"
      ],
      "metadata": {
        "id": "5_zqsIpNctGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Optimization\n"
      ],
      "metadata": {
        "id": "MglICYtchF4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview.generative_models import PromptOptimizer\n",
        "optimizer = PromptOptimizer()\n",
        "\n",
        "optimized_prompt = optimizer.optimize(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    task_type=\"text-generation\",\n",
        "    prompt=\"Explain cloud computing in simple terms.\",\n",
        "    optimization_goal=\"improve conciseness and clarity\"\n",
        ")\n",
        "print(\"Optimized prompt:\", optimized_prompt.optimized_prompt_text)\n"
      ],
      "metadata": {
        "id": "j-NJxKbthD_E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-evaluate optimized prompt\n",
        "optimized_data = pd.DataFrame({\n",
        "    'prompt': [optimized_prompt.optimized_prompt_text],\n",
        "    'response': [\"Cloud computing is using online servers to store and process data instead of your computer.\"],\n",
        "    'reference': [\"Cloud computing uses internet-based servers instead of local ones.\"]\n",
        "})\n",
        "optimized_data.to_json(\"eval_optimized.jsonl\", orient=\"records\", lines=True)\n"
      ],
      "metadata": {
        "id": "5CJmu-z2hOgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_eval = EvalTask(\n",
        "    dataset=\"eval_optimized.jsonl\",\n",
        "    metrics=[readability_metric, \"bleu\", \"rouge_l_sum\"],\n",
        "    experiment=\"prompt_re_eval_experiment\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Cq_xgRPfhOY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-DEWyHpb9W5"
      },
      "outputs": [],
      "source": [
        "optimized_result = re_eval.evaluate(\n",
        "    model=\"projects/your-project/locations/us-central1/models/YOUR_MODEL_ID\",\n",
        "    prompt_template=\"{prompt}\"\n",
        ")\n",
        "print(\"Re-evaluation completed. Metrics:\")\n",
        "print(optimized_result.metrics_summary)\n"
      ]
    }
  ]
}