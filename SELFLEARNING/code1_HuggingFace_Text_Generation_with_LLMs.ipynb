{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKB3HJRqq_M8"
      },
      "source": [
        "# SELFLEARNING HuggingFace: Using Pre-trained LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQUotuMgYaMb"
      },
      "source": [
        "### LLM: GPT-2\n",
        "\n",
        "Context: https://huggingface.co/openai-community/gpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ggocMJ4_Ypx"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='openai-community/gpt2')\n",
        "set_seed(42)\n",
        "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1QRBBpwYfdM"
      },
      "source": [
        "### Microsoft Phi2\n",
        "\n",
        "\n",
        "Phi-2 is a Transformer with 2.7 billion parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value). When assessed against benchmarks testing common sense, language understanding, and logical reasoning, Phi-2 showcased a nearly state-of-the-art performance among models with less than 13 billion parameters.\n",
        "\n",
        "\n",
        "HF: https://huggingface.co/microsoft/phi-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puu3YYoQYXKP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8RTPPb9fGen"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load model to GPU (cuda) if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load tokenizer and model from Hugging Face\n",
        "model_id = \"microsoft/phi-2\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if device == \"cuda\" else torch.float32)\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPWmMPh3gEz5"
      },
      "outputs": [],
      "source": [
        "# Your prompt\n",
        "prompt = \"Explain quantum computing in simple terms.\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "# Decode and print\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNHfVWe4ylf7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0GluZ3hyn7P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2282761"
      },
      "source": [
        "### DeepSeek R1-Distill-Qwen-1.5B\n",
        "\n",
        "DeepSeek R1-Distill-Qwen-1.5B is a distilled version of the Qwen-1.5B model. Distillation is a technique where a smaller model (the student) is trained to mimic the behavior of a larger, more powerful model (the teacher). This often results in a smaller, faster model that retains much of the performance of the larger model.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "*   **Distilled Model:** Benefits from the knowledge of a larger model while being more efficient.\n",
        "*   **Qwen-1.5B Base:** Built upon the Qwen-1.5B architecture.\n",
        "*   **Text Generation:** Capable of generating text based on provided prompts.\n",
        "*   **Parameters:** 1.5 billion\n",
        "*   **Input Modality:** Text\n",
        "*   **Output Modality:** Text\n",
        "*   **Input Context Window Size:** Not explicitly stated, but likely similar to Qwen-1.5B (around 32k tokens).\n",
        "*   **Output Context Window Size:** Not explicitly stated.\n",
        "*   **Training Data:** Distilled from Qwen-1.5B, which was trained on a large corpus of text and code data.\n",
        "*   **Organization:** DeepSeek AI\n",
        "\n",
        "**Use Cases:**\n",
        "\n",
        "*   Text generation tasks where efficiency is important.\n",
        "*   Applications on devices with limited computational resources.\n",
        "\n",
        "**Hugging Face:** [https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-JgtfUmylQd"
      },
      "outputs": [],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3c96a28"
      },
      "source": [
        "### Qwen/Qwen3-0.6B\n",
        "\n",
        "Qwen3-0.6B is a 0.6 billion parameter language model from the Qwen family of models. These models are known for their strong performance across various benchmarks.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "*   **Parameters:** 0.6 billion\n",
        "*   **Input Modality:** Text\n",
        "*   **Output Modality:** Text\n",
        "*   **Input Context Window Size:** Not explicitly stated, but typically large for Qwen models.\n",
        "*   **Output Context Window Size:** Not explicitly stated.\n",
        "*   **Training Data:** Trained on a large corpus of text and code data.\n",
        "*   **Organization:** Qwen\n",
        "\n",
        "**Use Cases:**\n",
        "\n",
        "*   Text generation tasks.\n",
        "*   Applications where a smaller, efficient model is suitable.\n",
        "\n",
        "**Hugging Face:** [https://huggingface.co/Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ8-6yfnzTpJ"
      },
      "outputs": [],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen3-0.6B\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60c3c05b"
      },
      "source": [
        "### TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
        "\n",
        "TinyLlama-1.1B-Chat-v1.0 is a compact language model with 1.1 billion parameters, designed for chat-based applications. It's a smaller model, making it more efficient for deployment in resource-constrained environments.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "*   **Parameters:** 1.1 billion\n",
        "*   **Input Modality:** Text\n",
        "*   **Output Modality:** Text\n",
        "*   **Input Context Window Size:** Not explicitly stated, but optimized for conversational contexts.\n",
        "*   **Output Context Window Size:** Not explicitly stated.\n",
        "*   **Training Data:** Trained on a large corpus of text data with a focus on conversational examples.\n",
        "*   **Organization:** TinyLlama\n",
        "\n",
        "**Use Cases:**\n",
        "\n",
        "*   Chatbots and conversational AI applications.\n",
        "*   Text generation in interactive scenarios.\n",
        "*   Deployment on devices with limited computational resources.\n",
        "\n",
        "**Hugging Face:** [https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1-_duqczdYe"
      },
      "outputs": [],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4h_P6m10jfl"
      },
      "source": [
        "## Multimodal LLMs\n",
        "\n",
        "### Microsoft Kosmos-2.5\n",
        "\n",
        "Kosmos-2.5 is a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCmLWhe8BBdR"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or9oODTVk-zO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "import requests\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "repo = \"microsoft/kosmos-2.5-chat\"\n",
        "device = \"cuda:0\"\n",
        "dtype = torch.bfloat16\n",
        "\n",
        "# sample image\n",
        "url = \"https://huggingface.co/microsoft/kosmos-2.5/resolve/main/receipt_00008.png\"\n",
        "\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU0KyyVTzpcQ"
      },
      "outputs": [],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"image-text-to-text\", model=\"microsoft/kosmos-2.5-chat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adWLmeMuBU8G"
      },
      "outputs": [],
      "source": [
        "question = \"What is the sub total of the receipt?\"\n",
        "template = \" USER: {} ASSISTANT:\"\n",
        "prompt = template.format(question)\n",
        "\n",
        "\n",
        "generated_text = pipe(images=image, text=prompt, max_new_tokens=1024)\n",
        "generated_text[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtw9yI_Z-MQ7"
      },
      "source": [
        "## Thank You"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}